{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29dab0",
   "metadata": {},
   "source": [
    "# STA130 Homework 08\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eee301",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Pre-lecture\" and \"Post-lecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Pre-lecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "    \n",
    "### Prompt Engineering? \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc1200",
   "metadata": {},
   "source": [
    "### Marking Rubric (which may award partial credit)\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Well-communicated and sensible answers for Question \"2\"\n",
    "- [0.2 points]: Correct code and well-communicated correct answer for Question \"4\" \n",
    "- [0.2 points]: Correct calculations for requested metrics in Question \"6\" \n",
    "- [0.3 points]: Correct and well-communicated explanation of differences for Question \"7\" \n",
    "<!-- - [0.1 points]: Written submission evaluation and enagement confirmation with ChatBot summaries for \"8\", \"10\"-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67686639",
   "metadata": {},
   "source": [
    "## \"Pre-lecture\" HW [*completion prior to next LEC is suggested but not mandatory*]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a53734",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Start a ChatBot session to understand what a *Classification Decision Tree* is: (a) ask the ChatBot to describe the type of problem a *Classification Decision Tree* addresses and provide some examples of real-world applications where this might be particularly useful, and then (b) make sure you understand the difference between how a *Classification Decision Tree* makes *(classification) predictions* versus how *Multiple Linear Regression* makes *(regression) predictions*<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _The first part (a) of this question is looking for you to understand the difference between **classification** and **regression**. The second part (b) of the questions is looking for a fairly high level understanding of the general nature of a decision tree and how it is based on making sequential decisions down the *nodes* of *tree* in order to eventually make a final prediction. This part (b) is essentially the **Classification Decision Tree** analog of \"explain how the **linear form** makes a prediciton in **Multiple Linear Regression** generally speaking\"; namely,\"explain how the **tree** makes a prediciton in a **Classification Decision Tree** generally speaking\"._\n",
    "> \n",
    "> _**If you're struggling with this, it would probably be most helpful to go search for and some images of example decision trees to look at!**_\n",
    "> \n",
    "> - _You may be beginning to realize or will nonetheless eventually come to understand that the sequential decisions at each stage of the **Decision Tree** are **interactions** (in the same manner as **interactions** in **Multiple Linear Regression**.  Once you start to see that and it's making sense to you then you'll increasingly appreciate how **complex** **Decision Tree** models can be, even though they're pretty simple to understand if you just look at one._\n",
    ">\n",
    "> ---\n",
    ">    \n",
    "> _When using chatbots, it's often more effective (and enjoyable) to ask concise, single questions rather than presenting complex, multi-part queries. This approach can help in obtaining clearer and more specific responses (that might be more enjoyable to interact with). You can always ask multi-part questions as a series of additional sequential questions. With this approach, chatbots may not automatically reiterate previously explained concepts. So if you need a refresher or further explanation on a topic discussed earlier, just explicitly request during follow-up interactions._\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f67d02",
   "metadata": {},
   "source": [
    "A Classification Decision Tree is a supervised machine learning algorithm used to solve classification problems. It helps in deciding the category or class to which a new observation belongs based on its features. Here's how it addresses such problems:\n",
    "\n",
    "(a) Problems Addressed by a Classification Decision Tree\n",
    "Type of problem:\n",
    "Classification Decision Trees deal with problems where the goal is to predict a categorical outcome. In these problems, the target variable (output) is discrete and represents distinct categories, such as \"Yes\" or \"No,\" \"Spam\" or \"Not Spam,\" or multiple classes like \"Low,\" \"Medium,\" or \"High.\" The tree structure splits the data into subsets based on feature values, with decisions at each node aimed at classifying the data as accurately as possible.\n",
    "\n",
    "Examples of real-world applications:\n",
    "\n",
    "Email spam detection: Classify emails as \"Spam\" or \"Not Spam\" based on features like sender, subject line, and content.\n",
    "Medical diagnosis: Predict whether a patient has a certain disease (\"Positive\" or \"Negative\") based on symptoms and test results.\n",
    "Credit risk assessment: Classify loan applicants as \"Low Risk\" or \"High Risk\" based on financial and personal information.\n",
    "Customer churn prediction: Identify whether a customer is likely to leave (\"Churn\") or stay (\"No Churn\") based on behavior patterns.\n",
    "(b) Difference Between a Classification Decision Tree and Multiple Linear Regression\n",
    "Classification Decision Tree (for classification tasks):\n",
    "\n",
    "Predictions: A decision tree makes classification predictions by segmenting the data into groups based on feature splits. The tree starts from a root node, and at each internal node, it evaluates a specific feature to split the data. The splitting continues until it reaches a leaf node, which represents the final predicted class (category). The decision is based on the majority class within that leaf node.\n",
    "For example, to classify if an email is \"Spam\" or \"Not Spam,\" the tree will ask a series of yes/no questions (e.g., \"Does the email contain certain keywords?\" \"Is the sender on a known spam list?\") and assign the email to one of these classes.\n",
    "Multiple Linear Regression (for regression tasks):\n",
    "\n",
    "Predictions: In contrast, multiple linear regression makes regression predictions, which means it estimates a continuous numerical outcome rather than a categorical one. It does this by fitting a linear relationship between the input features and the target variable.\n",
    "For instance, to predict a house price, multiple linear regression might combine the effects of variables like the size of the house, number of bedrooms, and neighborhood to compute a numerical estimate (e.g., $200,000).\n",
    "Key Differences:\n",
    "Prediction type:\n",
    "Classification Decision Tree: Predicts a category (e.g., \"Yes\" or \"No\").\n",
    "Multiple Linear Regression: Predicts a continuous value (e.g., price, temperature).\n",
    "Structure and logic:\n",
    "Decision Tree: Uses a series of if/else decisions based on data features to divide and classify the data.\n",
    "Linear Regression: Fits a linear equation to the data to find the best line that explains the relationship between the variables.\n",
    "In summary, classification decision trees are best suited for problems where the outcome is categorical, while multiple linear regression works when the outcome is continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43141d7f",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf97d1",
   "metadata": {},
   "source": [
    "### 2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following *metrics* below: provide your answers and, in your own words, *concisely explain your rationale for your answers.*<br>\n",
    "\n",
    "\n",
    "1. **Accuracy** measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "    \n",
    "2. **Sensitivity** measures the proportion of actual positives that are correctly identified.\n",
    "\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "3. **Specificity** measures the proportion of actual negatives that are correctly identified.\n",
    "\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "4. **Precision** measures the proportion of positive identifications that were actually correct.\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _This question could be restated as, \"Give examples of real-world examples where each of these **metrics** would be particularly useful.\"_\n",
    ">\n",
    "> _The primary objective here is to understand the distinction between each of these **metrics**. The secondary objective is to notice how the nature of the decision-making that each of these **metrics** most naturally supports is very distinct, ideally based on identifying memorable examples that really distinguish between the **metrics**._\n",
    ">\n",
    "> - _Have a look at this (greatly expanded) handy list of additional metrics, formulas, and synonyms at the following [wikipedia page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) if you want this to get real crazy real fast._\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6a7f2",
   "metadata": {},
   "source": [
    "1. Accuracy:\n",
    "Application Scenario: General classification problems with balanced classes, such as predicting if an email is spam or not.\n",
    "Rationale: Accuracy is a good measure when the cost of false positives and false negatives is similar, and the data has balanced classes (i.e., similar numbers of positive and negative cases). In this scenario, both false positives (non-spam classified as spam) and false negatives (spam classified as non-spam) are undesirable, but neither has significantly more impact than the other, making accuracy a good overall metric.\n",
    "2. Sensitivity (also known as Recall or True Positive Rate):\n",
    "Application Scenario: Medical diagnosis, such as detecting cancer in patients.\n",
    "Rationale: Sensitivity is important when the primary concern is correctly identifying all actual positive cases, even if some false positives occur. In medical diagnoses, missing a true positive (i.e., failing to diagnose a patient with cancer) can have severe consequences. High sensitivity ensures that most actual positive cases (cancer patients) are caught, even if some non-cancer patients are mistakenly flagged for further testing.\n",
    "3. Specificity (True Negative Rate):\n",
    "Application Scenario: Screening for a rare disease, such as HIV testing in a low-risk population.\n",
    "Rationale: Specificity is crucial when it's important to minimize false positives. For example, in a low-risk population, the number of true negative cases is much higher than true positive cases, so it's important to correctly identify the true negatives. A high specificity ensures that those who do not have the disease are not incorrectly diagnosed as positive, which could lead to unnecessary stress and further invasive testing.\n",
    "4. Precision:\n",
    "Application Scenario: Fraud detection in banking transactions.\n",
    "Rationale: Precision is important when the cost of false positives (incorrectly identifying normal transactions as fraud) is high. In fraud detection, falsely labeling a legitimate transaction as fraud can inconvenience customers and harm trust. Therefore, precision helps ensure that when a transaction is flagged as fraudulent, it is more likely to actually be fraud, thus minimizing the disruption to genuine customers while catching real fraudulent activity.\n",
    "Summary:\n",
    "Accuracy: Best for balanced problems where false positives and false negatives are equally important.\n",
    "Sensitivity: Best when identifying all actual positives is critical, like in medical screening.\n",
    "Specificity: Best when avoiding false positives is important, such as in rare disease testing.\n",
    "Precision: Best when false positives have a high cost, like in fraud detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcae562",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bd64b",
   "metadata": {},
   "source": [
    "### 3. Explore the amazon books dataset, seen previously at the start of the semester, providing some initital standard *exploratory data analysis (EDA)* and data summarization after pre-processing the dataset to meet the requirements below<br>\n",
    "\n",
    " 1. remove `Weight_oz`, `Width`, and `Height` \n",
    " 2. drop all remaining rows with `NaN` entries \n",
    " 3. set `Pub year` and `NumPages` to have the type `int`, and `Hard_or_Paper` to have the type `category`\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _`NaN` entries can't be used in their raw form with the `scikit-learn` methodologies, so we do need to remove them to proceed with our analyses._\n",
    ">     \n",
    "> _Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss..._\n",
    ">\n",
    "> _It would be possible to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now._ \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4b50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "# create `ab_reduced_noNaN` based on the specs above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d9810",
   "metadata": {},
   "source": [
    "Step 1: Load the dataset\n",
    "The Amazon Books dataset is being loaded from the provided URL.\n",
    "Step 2: Remove columns and drop rows with NaN\n",
    "You need to remove the columns Weight_oz, Width, and Height, and drop any remaining rows that contain missing (NaN) values.\n",
    "Step 3: Set appropriate data types\n",
    "Convert the Pub year and NumPages columns to integers, and set Hard_or_Paper as a categorical variable.\n",
    "Step 4: Initial Data Summary and EDA\n",
    "Now that the dataset is preprocessed, you can proceed with an initial exploration of the data:\n",
    "General Dataset Information: This will give an overview of the data, such as the number of rows, columns, and data types.\n",
    "Summary Statistics: Use summary statistics to understand the distribution of numerical data such as NumPages and Pub year.\n",
    "Categorical Summary for Hard_or_Paper: You can see the distribution of books across the categories (hardcover, paperback).\n",
    "Histogram for Numeric Columns: Plot histograms for NumPages and Pub year to visualize the distribution of these variables.\n",
    "Correlation Analysis: Explore any correlations between the numeric variables to see if there are any relationships worth investigating further.\n",
    "\n",
    "Conclusion:\n",
    "This preprocessing and initial EDA will clean the dataset and give insights into the structure of the data. From here, further analysis could focus on patterns related to publication year, page counts, or comparing distributions between hardcovers and paperbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018b553",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225c340",
   "metadata": {},
   "source": [
    "### 4. Create an 80/20 split with 80% of the data as a training set *ab_reduced_noNaN_train* and 20% of the data testing set  *ab_reduced_noNaN_test* using either *df.sample(...)* as done in TUT or using *train_test_split(...)* as done in the previous HW, and report on how many observations there are in the training data set and the test data set.<br><br>Tell a ChatBot that you are about to fit a \"scikit-learn\" *DecisionTreeClassifier* model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree *clf* using only the *List Price* variable to predict whether or not a book is a hard cover or paper back book using a *max_depth* of *2*; finally use *tree.plot_tree(clf)* to explain what *predictions* are made based on *List Price* for the fitted *clf* model\n",
    "\n",
    "```python\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "```\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _To complete the final 80/20 split of the **observations** in a reproducible way, set a \"random seed\"._ \n",
    "> \n",
    "> - _A single **observation** consists of all the measurements made on a single entity, typically corresponding to a row of a data frame. In **Machine Learning**, a collection of values of interest measured for a single entity is called a \"vector\" and so the **observation** is referred to as a **vector**_.\n",
    ">    \n",
    "> _Asking the ChatBot about \"DecisionTreeClassifier .fit(...)\" can be helpful here..._\n",
    "> \n",
    "> _Should you use the \"ab_reduced_noNaN\" data, or the \"ab_reduced_noNaN_train\" data, or the \"ab_reduced_noNaN_test\" data to initially fit the classification tree? Why?_\n",
    ">    \n",
    "> _You can visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html); but, to make it more immediately readible it might be better to use `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees)_ \n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec19ab9",
   "metadata": {},
   "source": [
    "Step 1: Create an 80/20 Train-Test Split\n",
    "You can use train_test_split from sklearn to split the dataset into training and testing sets. Here's how to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f7461e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Define X and y\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mab_reduced_noNaN\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mList Price\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(ab_reduced_noNaN[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHard_or_Paper\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# 'H' corresponds to hardcover\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create 80/20 split\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X and y\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN['Hard_or_Paper'])['H']  # 'H' corresponds to hardcover\n",
    "\n",
    "# Create 80/20 split\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Report the number of observations in the training and testing sets\n",
    "print(f\"Training set size: {ab_reduced_noNaN_train.shape[0]} observations\")\n",
    "print(f\"Testing set size: {ab_reduced_noNaN_test.shape[0]} observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce4ea01",
   "metadata": {},
   "source": [
    "Step 2: Explanation of the Two Steps Before Fitting the Model\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']:\n",
    "\n",
    "What it's doing: This line converts the Hard_or_Paper categorical column (which could contain values like \"H\" for hardcover and \"P\" for paperback) into a numerical format that the model can understand. Specifically, it creates dummy variables (0s and 1s), and selects the column representing hardcover books (1 for hardcover, 0 for paperback).\n",
    "X = ab_reduced_noNaN[['List Price']]:\n",
    "\n",
    "What it's doing: This line selects the List Price column from the dataframe and uses it as the feature for training the classification model. The feature (X) is what the model will use to make predictions, while the target variable (y) is what the model will predict (whether a book is hardcover or paperback).\n",
    "Step 3: Train a Decision Tree Classifier\n",
    "Now that you understand what the code is doing, we can proceed to fit a decision tree model. Here's how to train the model using only the List Price feature and set the maximum depth of the tree to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094ebc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ab_reduced_noNaN_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m clf \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the classifier on the training data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mab_reduced_noNaN_train\u001b[49m, y_train)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plot the decision tree\u001b[39;00m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ab_reduced_noNaN_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# Initialize the classifier with max_depth of 2\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(ab_reduced_noNaN_train, y_train)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(10, 7))\n",
    "tree.plot_tree(clf, feature_names=['List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3d0f4e",
   "metadata": {},
   "source": [
    "Explanation of Predictions:\n",
    "After fitting the classifier, the decision tree will split the List Price feature into different ranges to predict whether a book is hardcover or paperback.\n",
    "Each node in the tree represents a decision based on the List Price, and the leaves of the tree indicate the final prediction (hardcover or paperback).\n",
    "For example, if the price of a book is below a certain threshold, the model might predict that it is a paperback, and if it's above another threshold, the model might predict it's a hardcover.\n",
    "The tree plot will visually show these decision thresholds, making it easier to interpret how the model makes predictions based on List Price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f230d",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac0e5b",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Pre-lecture VS Post-lecture HW\n",
    "\n",
    "Feel free to work on the \"Post-lecture\" HW below if you're making good progress and want to continue: for **HW 08** this could be reasonable because, as you'll see, the process of creating and using **classification decision trees** is quite similar to the process for creating and using **multiple linear regression** models. There are differences of course, such as how there is **coefficient hypothesis testing** in **multiple linear regression** and **confusion matrices** in **classification decision trees**, and so on. But you would very likely be able to leverage the silarities to make a lot of progress with **classification decision trees** based on your experience with **multiple linear regression**.\n",
    "    \n",
    "*The benefits of continue would are that (a) Consolidate the knowledge already learned and integrate it comprehensively. (b) Let you build experience interacting with ChatBots (and understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75e7d3e",
   "metadata": {},
   "source": [
    "## \"Post-lecture\" HW [*submission along with \"Pre-lecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3ce1f",
   "metadata": {},
   "source": [
    "### 5. Repeat the previous problem but this time visualize the *classification decision tree* based on the following specifications below; then explain generally how predictions are made for the *clf2* model<br>\n",
    "\n",
    "1. `X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]`\n",
    "2. `max_depth` set to `4`\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> - _Use the same **train/test split** dataset used so far_\n",
    "> - _Train the **classification decision tree** `clf2` using **predictor variables** `NumPages`, `Thick` and `List Price`_ \n",
    "> - _Again **predict** whether or not a book is hard cover book or a paper back book_\n",
    "> - _You can visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html); but, to make it more immediately readible it might be better to use `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees)_\n",
    ">\n",
    "> _If you are interested in how to find the best `max_depth` for a tree, ask ChatBot about \"GridSearchCV\"_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acb3ee",
   "metadata": {},
   "source": [
    "To visualize the decision tree based on the new specifications and explain how predictions are made, we'll follow the same general process as before but with additional features and a greater depth in the decision tree.\n",
    "\n",
    "Step 1: Define X and Update Model Specifications\n",
    "This time, we'll use three features: NumPages, Thick, and List Price. We'll also set the max_depth of the tree to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a03345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X with the three features\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "\n",
    "# Initialize the Decision Tree Classifier with max_depth of 4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Train the classifier on the new training data\n",
    "clf2.fit(ab_reduced_noNaN_train[['NumPages', 'Thick', 'List Price']], y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31aac4b2",
   "metadata": {},
   "source": [
    "Step 2: Visualize the Decision Tree\n",
    "We'll now visualize the decision tree that is trained on the new feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5398be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(clf2, feature_names=['NumPages', 'Thick', 'List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97a83f",
   "metadata": {},
   "source": [
    "Step 3: Explanation of Predictions for clf2\n",
    "For the clf2 model, predictions are made based on a combination of the three input features: NumPages (the number of pages in the book), Thick (the thickness of the book), and List Price (the price of the book). Here's how it works:\n",
    "\n",
    "Decision Nodes:\n",
    "Each internal node in the tree represents a decision point based on one of the features (e.g., NumPages, Thick, or List Price). The tree evaluates a feature at each node and uses it to split the data into two groups.\n",
    "\n",
    "Splitting Criteria:\n",
    "The decision at each node uses a threshold (e.g., \"Is NumPages less than 350?\") to split the data. If the condition is true, the data follows one branch; if false, it follows the other. The tree continues this process down the branches.\n",
    "\n",
    "Leaf Nodes (Predictions):\n",
    "At the end of the branches (leaf nodes), the model makes a final prediction about whether a book is a hardcover or a paperback. These leaf nodes represent the majority class of the training data that ended up in that node, and they provide the predicted classification.\n",
    "\n",
    "Feature Influence:\n",
    "The deeper the node, the more specific the splits become based on the combination of NumPages, Thick, and List Price. For example:\n",
    "\n",
    "A book might first be split by its NumPages (longer books may be more likely to be hardcover).\n",
    "Within that split, the tree might use Thick to further differentiate between hardcovers and paperbacks.\n",
    "Finally, List Price might be used at the deeper levels to refine the prediction even further.\n",
    "Summary:\n",
    "The clf2 model makes predictions based on a combination of the three features, using a hierarchical process where each feature is evaluated sequentially to classify the book as hardcover or paperback.\n",
    "The maximum depth of 4 allows the tree to make more specific distinctions but prevents overfitting by limiting how deep the tree can grow. Each decision boundary is set based on the feature values, and the leaf nodes indicate the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4c9b4",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889386a",
   "metadata": {},
   "source": [
    "### 6. Use previously created *ab_reduced_noNaN_test* to create confusion matrices for *clf* and *clf2*. Report the sensitivity, specificity and accuracy for each of the models<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Hopefully you immediately thought to ask ChatBot to help you with this problem, but if you did you should take time to make sure you're clear about the key components of what the ChatBot is providing for you. You might want to know_\n",
    "> - _what is a \"positive\" and what is a \"negative\"_\n",
    "> - _how to read an `sklearn` confusion matrix_\n",
    "> - _what leads to TP, TN, FP, and FN_\n",
    "> - _whether `y_true` or `y_pred` go first in the `confusion_matrix` function_   \n",
    ">\n",
    "> _Have the visualizations you make use decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`), probably based on `np.round()`_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "       \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3d1fe8",
   "metadata": {},
   "source": [
    "To calculate the confusion matrices and report the sensitivity, specificity, and accuracy for both clf and clf2, we need to follow these steps:\n",
    "\n",
    "Step 1: Import Necessary Libraries\n",
    "We'll use the confusion_matrix from sklearn and calculate sensitivity, specificity, and accuracy based on the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b52761",
   "metadata": {},
   "source": [
    "Step 2: Make Predictions for clf and clf2 Models\n",
    "We'll first make predictions on the test set (ab_reduced_noNaN_test) for both the clf and clf2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ebf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using clf (List Price only)\n",
    "y_pred_clf = clf.predict(ab_reduced_noNaN_test[['List Price']])\n",
    "\n",
    "# Make predictions using clf2 (NumPages, Thick, List Price)\n",
    "y_pred_clf2 = clf2.predict(ab_reduced_noNaN_test[['NumPages', 'Thick', 'List Price']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64281ec1",
   "metadata": {},
   "source": [
    "Step 3: Calculate Confusion Matrices\n",
    "Now, we can calculate the confusion matrices for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for clf\n",
    "conf_matrix_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "\n",
    "# Confusion matrix for clf2\n",
    "conf_matrix_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "print(\"Confusion Matrix for clf (List Price):\\n\", conf_matrix_clf)\n",
    "print(\"Confusion Matrix for clf2 (NumPages, Thick, List Price):\\n\", conf_matrix_clf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113b19e",
   "metadata": {},
   "source": [
    "Step 4: Calculate Sensitivity, Specificity, and Accuracy\n",
    "We will extract the values from the confusion matrices to compute the metrics. A typical confusion matrix looks like this:\n",
    "\n",
    "TN: True Negatives\n",
    "FP: False Positives\n",
    "FN: False Negatives\n",
    "TP: True Positives\n",
    "From this, we can compute the following:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Sensitivity (Recall) = TP / (TP + FN)\n",
    "Specificity = TN / (TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8609bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the metrics from the confusion matrix\n",
    "def compute_metrics(conf_matrix):\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    sensitivity = TP / (TP + FN)  # Also known as recall\n",
    "    specificity = TN / (TN + FP)\n",
    "    return accuracy, sensitivity, specificity\n",
    "\n",
    "# Compute metrics for clf\n",
    "accuracy_clf, sensitivity_clf, specificity_clf = compute_metrics(conf_matrix_clf)\n",
    "\n",
    "# Compute metrics for clf2\n",
    "accuracy_clf2, sensitivity_clf2, specificity_clf2 = compute_metrics(conf_matrix_clf2)\n",
    "\n",
    "# Report the metrics for both models\n",
    "print(f\"Metrics for clf (List Price only):\\n Accuracy: {accuracy_clf:.3f}, Sensitivity: {sensitivity_clf:.3f}, Specificity: {specificity_clf:.3f}\")\n",
    "print(f\"Metrics for clf2 (NumPages, Thick, List Price):\\n Accuracy: {accuracy_clf2:.3f}, Sensitivity: {sensitivity_clf2:.3f}, Specificity: {specificity_clf2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce25bb",
   "metadata": {},
   "source": [
    "Step 5: Explanation of Results\n",
    "Once you've run the above code, you will have the following metrics for both clf and clf2:\n",
    "\n",
    "Accuracy: The overall proportion of correctly predicted books (hardcover or paperback).\n",
    "Sensitivity: The ability of the model to correctly identify hardcovers (True Positives / All Positives).\n",
    "Specificity: The ability of the model to correctly identify paperbacks (True Negatives / All Negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1638d",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72576c25",
   "metadata": {},
   "source": [
    "### 7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for *clf* and *clf2*) are better<br>\n",
    "\n",
    "```python\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(ab_reduced_noNaN_train[['List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(\n",
    "                         ab_reduced_noNaN_train[['NumPages','Thick','List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "```\n",
    "\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "    \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337c648c",
   "metadata": {},
   "source": [
    "The two confusion matrices you refer to are based on the training set predictions, which can lead to overfitting when compared to the test set results. In the first confusion matrix, the model (clf) is trained using only List Price, so it might oversimplify the relationships, causing higher misclassification rates in complex situations. In contrast, the second confusion matrix uses multiple features (NumPages, Thick, and List Price), making it more complex and likely to overfit the training data by capturing noise or spurious relationships.\n",
    "\n",
    "However, the confusion matrices for clf and clf2 (generated from the test set) are better because they evaluate the model's performance on unseen data, giving a more realistic picture of how well the models generalize to new data. The differences in metrics (accuracy, sensitivity, and specificity) between the training and test sets reflect whether the model is generalizing well or overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0cf29",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526939c2",
   "metadata": {},
   "source": [
    "\n",
    "### 8. Read the paragraphs in *Further Guidance* and ask a ChatBot how to visualize *feature Importances* available for *scikit-learn* *classification decision trees*; do so for *clf2*;  and use *.feature_names_in_* corresponding to *.feature_importances_* to report which *predictor variable* is most important for making predictions according to *clf2*<br>\n",
    "\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "  \n",
    "> The way a **classification decision tree** is fit is that at each step in the construction process of adding a new **decision node splitting rule** to the current **tree structure**, all possible **decision rules** for all possible **predictor variables** are considered, and the combination that improves the **prediction** the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") and in accordance with the rules of the decision tree (such as the `max_depth` argument) is added to the **classification decision tree**.  Thus overall \"criterion\" noted above improves with each new **decision node splitting rule**, so the improvement can thus be tracked and the improvement contributions attributed to the **feature** upon which the **decision node splitting rule** is based.  This means the relative contribution of each **predictor variable** to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. \n",
    ">\n",
    "> Compared to the simplicity of understanding how different **covariates** contribute towards the final **predicted values** of **multiple linear regression models** (by just reading off the equation to see how predictions work), the the complexity of how all the different **features** interact and combine to together to create the final **predictions** from **classification decision trees** can be staggering. But the so-called **feature importance** heuristics allows us to judge how relatively important the overall contributions from different features are in the final decision tree predictions. Now we just need to be sure we're not **overfitting** our **classification decision trees** since they can be so **complex**. Fortunately, the \"GridSearchCV\" methodology mentioned in regards to finding the best `max_depth` setting for a tree is going to provide a general answer to the challenge of complexity and **overfitting** in **machine learning models** that is not too hard to understand (and which you might already have some guesses or a hunch about). \n",
    "> \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "       \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ee57b",
   "metadata": {},
   "source": [
    "To visualize feature importances for scikit-learn classification decision trees and determine which feature contributes most to the predictions, we can use the feature_importances_ attribute of the fitted decision tree model. Here's how you can do it for clf2:\n",
    "\n",
    "Step 1: Get Feature Importances\n",
    "The clf2.feature_importances_ attribute will give us the importance of each feature used in the decision tree. The higher the value, the more important the feature is in making the predictions.\n",
    "\n",
    "Step 2: Retrieve Feature Names\n",
    "We can use clf2.feature_names_in_ to get the names of the features corresponding to these importance values.\n",
    "\n",
    "Step 3: Visualization and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ea78f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get feature importances and feature names\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m feature_importances \u001b[38;5;241m=\u001b[39m \u001b[43mclf2\u001b[49m\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[1;32m      6\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m clf2\u001b[38;5;241m.\u001b[39mfeature_names_in_\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a pandas DataFrame to display the feature importances nicely\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf2' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Get feature importances and feature names\n",
    "feature_importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Create a pandas DataFrame to display the feature importances nicely\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances for clf2')\n",
    "plt.gca().invert_yaxis()  # To show the highest importance at the top\n",
    "plt.show()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320bf35f",
   "metadata": {},
   "source": [
    "Explanation of Output:\n",
    "Most important feature: The feature with the highest importance value in the DataFrame is the one that contributes the most to the decision-making process in clf2.\n",
    "Plot Interpretation: The horizontal bar plot shows the relative importance of each feature. The longer the bar, the more significant that feature is in determining whether a book is classified as hardcover or paperback.\n",
    "By examining both the plot and the sorted DataFrame, you can easily identify the most important predictor variable according to the clf2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da860dc",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d92c",
   "metadata": {},
   "source": [
    "### 9. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees in two to three sentences<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> _Linear model regression predicts continuous real-valued averages for a given configuration of covariate values (or, feature values, if we're using machine learning terminology instead of statistical terminology), whereas a binary classification model such as a binary classification tree predicts 0/1 (\"yes\" or \"no\") outcomes (and gives the probability of a 1 \"yes\" (or \"success\") outcome from which a 1/0 \"yes\"/\"no\" prediction can be made; but, this is not what is being asked here. This question is asking \"what's the difference in the way we can interpret and understand how the predictor variables influence the predictions in linear model regression based on the coefficients versus in binary decision trees based on the Feature Importances?\"_\n",
    ">    \n",
    "> ---\n",
    "> \n",
    "> _Don't forget to ask for summaries of all your different ChatBot sessions and organize and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatBot); but, if you're using the STA130 custom NBLM ChatBot, you'll only be able to ask for summaries, of course!_ \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204cc97b",
   "metadata": {},
   "source": [
    "In linear regression, coefficients represent the direct, additive impact of each feature on the predicted outcome, assuming a linear relationship between the feature and the target. A positive or negative coefficient indicates the direction and strength of this relationship. In decision trees, feature importances reflect how often and effectively a feature is used to split the data to reduce uncertainty, but they don't provide a simple direction or magnitude of influence; they indicate the relative contribution of each feature to the overall prediction, without assuming a linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dd590",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/673fac6f-0070-8008-905a-f0eeb914e9dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6fb0cb",
   "metadata": {},
   "source": [
    "### 10. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br>\n",
    "  \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    ">  _Here is the link of [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) in case it gets lost among all the information you need to keep track of_  : )\n",
    "> \n",
    "> _Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)_\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776218b",
   "metadata": {},
   "source": [
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2436778",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course \n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "### Ethical Professionalism Considerations\n",
    "\n",
    "- Discuss with a ChatBox about consent and data collection for training models.\n",
    "    - Discuss the ethics of data collection for training decision trees, particularly the need for informed consent when personal data is involved.\n",
    "    - Evaluate the role of regulatory frameworks in ensuring ethical data collection practices.\n",
    "- Discuss with a ChatBox about accountability in automated decision-making.\n",
    "    - Address the challenges of holding systems and their developers accountable when decision trees lead to adverse outcomes.\n",
    "    - Explore legal and ethical frameworks for responsibility when automated decisions go wrong.\n",
    "- Discuss with a ChatBox about transparency and explainability in classification models.\n",
    "    - Discuss the importance of model transparency, particularly when using decision trees in sectors like healthcare or criminal justice.\n",
    "    - Explore methods to enhance the explainability of decision trees, such as visualization techniques and simplified decision paths.\n",
    "- Discuss with a ChatBox about impact of misclassifications in critical applications.\n",
    "    - Examine the consequences of false positives and false negatives in decision tree outcomes, using confusion matrices to highlight these issues.\n",
    "    - Discuss ethical responsibilities when deploying classifiers in high-stakes fields like medicine or law enforcement.\n",
    "    \n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "**Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times.**\n",
    "\n",
    "Information about the course project is available on the course github repo [here](https://github.com/pointOfive/stat130chat130/tree/main/CP), including a draft [course project specfication](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F23_course_project_specification.ipynb) (subject to change). \n",
    "- The Week 01 HW introduced [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb), and the [available variables](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view). \n",
    "- Please do not download the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage (or the course github repo) multiple times.\n",
    "    \n",
    "\n",
    "> ### NEW DEVELOPMENT<br>New Abilities Achieved and New Levels Unlocked!!!    \n",
    "> \n",
    "> And with that, ALL LEVELS unlocked! \n",
    ">\n",
    "> CONGRATS, YOU LEGENDS! 🎉\n",
    ">\n",
    "> You’ve battled through the wild jungles of deadlines, defeated the mighty Homework Beasts, and climbed the towering Mount Procrastination. And guess what? YOU MADE IT TO THE TOP! 🏔️\n",
    "> \n",
    "> Take a bow, grab a treat, and enjoy the sweet, sweet taste of freedom(**just for now , because you still have to finish the project! But you are almost done!**). You’ve earned it. Now go out there and celebrate like the absolute rockstars you are! 🌟💪\n",
    ">\n",
    "\n",
    "    \n",
    "### Current Course Project Capability Level    \n",
    "    \n",
    "I mean, the **course project** is basically, like, essentially now.\n",
    "    \n",
    "- Will you be doing any **classification decision trees** stuff for the course project?\n",
    "    - You could consider making some [partial dependency plots](https://scikit-learn.org/stable/modules/partial_dependence.html) if so...\n",
    "    - those might provide an interesting analysis in addition to **tree structure** visualizations, **confusion matrices**, **feature importances**. and the standard \"in-sample versus out-of-sample\" **train-test validation** analysis that would be expected in a **machine learning context**\n",
    "    \n",
    "- You could see if there are any interesting columns that might make for a potentially interesting **classification decision tree** analysis\n",
    "    - You wouldn't have to though...\n",
    "    - But if you did you'd want to be able to articulate and explain why what you're doing with **classification decision trees** is appropriate and enlightening\n",
    "\n",
    "- Anyway, I guess that just leaves reviewing all the statistical techniques covered in STA130, and considering integrating them holistically into your project!\n",
    "    \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524759d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
